---
title: "Databases are an effective and efficient method for storage and access of mass-spectrometry data"
author:
  - William Kumler
  - Anitra E. Ingalls
output: word_document
bibliography:
- Exported Items.bib
csl: journal-of-proteome-research.csl
---

```{r setup, include=FALSE}
options(timeout=3600)
options(pillar.sigfig=7)
options(digits=10)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

https://researcher-resources.acs.org/publish/author_guidelines?coden=jprobs

# Abstract

(200 words max)

# Introduction

Mass spectrometry (MS) still lacks a performant data access format. The mzML file type [@Martens2011], a result of over a decade of interlaboratory collaboration and workshopping, struggles to provide rapid computational access to the m/z and intensity tuples. This is the crucial component in nearly all mass spectrometry analysis, but mzML’s text-based XML format requires time-consuming decompression performed one scan at a time. This is largely due to its preservation of the scan as the unit of transaction while the field moves increasingly away from single-scan analysis [@Rost2014; @Ting2015].

As a result, alternative file formats are proposed practically every year. These include direct improvements to the mzML format with indexing [@Rost2015] and better internal encoding of the data [@Bhamber2021], HDF5-based alternatives [@Bhamber2021; @Wilhelm2012; @Bilbao2023; @Tully2020; @Askenazi2017], relational databases [@Shah2010; @Bouyssie2015; @Handy2017; @Yang2022; @Beagley2009], or fully custom alternatives [@Rompp2011; @Lu2022]. Fundamentally, these alternatives implement tradeoffs between user sanity in exchange for access speed and/or size on disk with clever compression algorithms and modern data structures that move away from the human-readable format of the mzML. These optimized formats are inherently more difficult to understand and usually lack comprehensive documentation or examples (particularly across programming languages) making it difficult for new users to enjoy their benefits or extend their functionality. This steep learning curve, coupled with a lack of support in conversion tools such as Proteowizard’s msconvert [@Chambers2012], has prevented widespread adoption of these new formats despite their clear computational advantages. Such formats are also fragile in the sense that without community support, their continued development depends entirely on the original developers and easily become deprecated (as is the case with YAFMS, Shaduf, and mz5, all of whom have links in their papers that currently redirect to missing webpages). A simple, speedy, and small MS data format remains very much in demand.

Relational databases are not new for MS workflows (see references above) and compete predominantly with HDF5-based methods. Both of these systems are widely used for big data and can be applied to MS data in a plethora of ways, leading to the proliferation of implementations we see today. Both backends provide excellent universality, larger-than-memory support, and rapid access to data, but HDF5-based systems excel at self-description and hierarchical structures [@Askenazi2017] while the relational database model is optimized for multi-table queries using a consistent syntax [@Cobb1970]. Relational databases are increasingly seen in MS workflows for both raw and processed data, with SQLite backends now supported in the popular peakpicking software xcms [@Smith2006] via the Spectra package [@Rainer2022] (though in-memory and HDF5 options are also supported) and on MetabolomicsWorkbench [@Sud2016] while the development of MassQL [@Jarmusch2022] demonstrates the increasing comfort that MS analysts have with the adoption of SQL.

Relational databases also have several distinct advantages over hierarchical or text-based systems, particularly in performing searches for subsets of data via indices. Importantly, this indexing differs from the byte-offset indexes that already exist in the indexed mzML and HDF5 formats because the search for a particular subset cannot be done efficiently with a byte-offset index when the m/z data is encoded, though access to a particular scan can be incredibly rapid. Additionally, data from multiple samples can be stored together in a single table, differing from formats like mzDB, mzTree, and mzMD and allowing queries of all dataset samples to be performed without looping through each file in turn, thereby avoiding the associated computational overhead and query complexity. 

SQL databases also allow mass spectrometrists to access the continual improvements and long-term stability produced by the industries who specialize in these. While HDF5 is a common scientific data format, databases are constantly under development by industry titans deeply invested in their maintenance and optimization. Online analytical processing (OLAP) methods are particularly well suited for MS data given their optimization for read speed under the assumption of infrequent transactions, making modern systems such as DuckDB [@Raasveldt2019] or Apache Arrow’s columnar formats highly appealing while preserving the familiar serverless approach.

Our previous work showed how the ragged arrays of MS data can be converted into a tidy database table in memory [@Kumler2022] and we now logically extend that method into proper database storage and access. Here, we test the hypothesis that a “vanilla” implementation of a relational database which exposes the raw m/z and intensity tuples is an intuitive and performant way of storing MS data for exploratory analysis, visualization, and quality control. We compare the time and space required to extract a representative data subset under three conditions (single scan, ion chromatogram, and all scans within a retention time range) and perform these tests on multiple databases as well as mzML and other MS data formats across multiple MS experiments with varying magnitudes for comparison.

# Experimental section

We performed a literature search for mass-spectrometry data formats that have been published in the last 15 years and attempted to find or construct parsers for each format in Python, a popular high-level interpreted language. For formats which had multiple parsers available (mzML), we performed intercomparisons between the different parsers and used the best one available for each query type.

Each parser was written to perform three common exploratory data analysis operations on full-scan data and four common operations on MS/MS fragmentation data. Full scan queries consisted of single scan extraction by scan number, retention time range extraction of all scans within a specified retention time range, and chromatogram extraction, which collects the ions within a specified parts-per-million (ppm) error of a known mass. These queries generally correspond to the methods used in @Bouyssie2015, which performed similar tests benchmarking the mzDB format against mz5 and an mzML parser. MS/MS queries involved four relevant queries: a single scan extraction by scan number similar to that of the full scan; extraction of all the fragments associated with a precursor *m/z* within a given ppm; extraction of all fragments with *m/z* values within a given ppm; and finally a neutral loss query computed by subtracting the fragment mass from the precursor mass where the difference between the two fell within a specified ppm window around a given *m/z*.

## Mass-spectrometry files

Metabolights search for
1. MS1 & MS2 data
2. .raw or .wiff file types (mzDB doesn't support .d or .mzML or .mzXML)
3. 100+ GB folder
4. Positive mode only(?)

Converted via msconvert or custom implementation

```{shell}
cd Documents\Will
git clone https://github.com/wkumler/mzsql
cd mzsql

conda create -n "mzsql_3_11" python=3.11 numpy=1.26.4
pip install https://pypi.cs.uni-tuebingen.de/packages/pyopenms-3.0.0.dev20230306-cp311-cp311-win_amd64.whl#md5=cdce650718bea07a0193b3227a2c075b
conda activate mzsql_3_11
pip install -e .
pip install git+https://github.com/PNNL-m-q/mzapy.git@no_full_mz_array#egg=mzapy

cd java_things
curl -O https://dlcdn.apache.org/maven/maven-3/3.9.9/binaries/apache-maven-3.9.9-bin.zip
tar -xf apache-maven-3.9.9-bin.zip
git clone https://github.com/optimusmoose/MZTree.git
cd MZTree
cd msDataServer
C:\Users\willi\Documents\mzsql\java_things\apache-maven-3.9.9\bin\mvn package
java -Dsun.java2d.d3d=false -Dsun.java2d.opengl=false -jar target\msDataServer-1.0.jar
(manually) open mztree file in port 4568

git clone https://github.com/yrm9837/mzMD-java.git
cd mzMD-java
C:\Users\willi\Documents\mzsql\java_things\apache-maven-3.9.9\bin\mvn package
java -Dsun.java2d.d3d=false -Dsun.java2d.opengl=false -jar target\msDataServer-1.0.jar
(manually) open mzmd file in port 4567
cd ..




E:
cd mzsql\MTBLS10066

msconvert *.raw --filter "peakPicking true 1-" --filter "polarity positive" 
# Also convert neg mode to see what benefit to offer pos/neg combos
msconvert *.raw --filter "peakPicking true 1-" --filter "polarity negative" -o "negative"
msconvert *.raw --mzMLb --filter "peakPicking true 1-" --filter "polarity positive" 
msconvert *.raw --mz5 --filter "peakPicking true 1-" --filter "polarity positive"

for %f in (*.raw) do "C:\Program Files\ProteoWizard\raw2mzDB_0.9.10_build20170802\raw2mzDB.exe" -i %f -c 1-2 --safeMode

for %f in (*.raw) do mza.exe -file %f
```

## Python-based file parsers

Comparisons between different mzML methods

Some details about the parsers I wrote, nuances of which can be looped and which can't


## Database schema

This “vanilla” database deprioritizes the metadata associated with a given file and focuses on performance and simplicity. Thus, this method does not replace the existing vendor-specific or mzML files but instead constructs a parallel data structure and represents our intuition that metadata is rarely the main target of MS analysis and that labs typically preserve the original files anyway.

Discuss how metadata fits into these parsers

Discuss links between various tables via scan_idx column?

$MS1

$MS2

$scan_info

$file_info

$sample_metadata

$rt_correction

$picked_peaks

## Time and space testing

Via %timeit% (need to redo these with randomization)

# Results

## Building parsers

Which file types we were able to find/write parsers for and our notes on their methodology

Good documentation for pyteomics, pyopenms, pymzml, mzmlb (at least via pyteomics)
Middling documentation for mza/mzapy, mzDB
Terrible documentation for Aird, MzTree, mzMD, mz5
Nonexistent documentation for YAFMS, Shaduf

pyopenms requires an old version of Python (3.11) and numpy (1.26.4) because of AVX support - discovered via GitHub issue with no requirements.txt file or environment.yml. Therefore requires using old version of the package.

Aird has zero documentation or application support (Python and C# versions are functionally deprecated)

Toffee only supports TOF data

MzTree and mzMD have no MS2+ support and don't support scan extraction
Neither of them handle multiple files

mz5 doesn't make it clear how to access the precursor masses for MS2 things so those parsers are missing, ref mzMLb paper here

mzDB handles metadata as XML strings and doesn't support bounding boxes for MS2 things afaics
Doesn't support .d files (only .raw and .wiff)
Doesn't handle polarity switching data

MZA can't perform centroiding simultaneously with .raw conversion, requires switch from .mzML
MZA doesn't support .wiff (only .raw or .d)
mzapy officially only supports .d (.raw requires installing from a separate development branch)
mzapy doesn't support direct scan extraction - baffling given how well the file type exposes this
mzapy doesn't index by MS2 things (only RT/DT)

Several fundamental inefficiencies identified:
1. Looping over each scan within a file
2. Looping over each file within a dataset
3. Decoding mz/int information each time if filtering on these fields
4. RTs are not accessible outside of looping over each scan for RT subsetting
5. Scan ids are not accessible outside of looping over each scan for scan extraction

## Timings for a single file

## Timings for a dataset with multiple files (and file sizes?)

## Database optimization 
1) Indices and ordering
2) Peak data extraction via joins vs loops vs unified query


# Discussion

While the gap between data scientist and mass spectrometrist continues to narrow, current implementations of MS data storage systems almost seem designed to *increase* it. This is accomplished by extremely poor documentation that makes it difficult to do anything beyond the original designer's intent, typically with data structures that are not familiar to the MS expert.

Not a single parser was without flaws - all were either slow, poorly documented, or had highly limited functionality

Querying things by scan number is dumb.
1) Different indices for pyopenms vs pymzml vs pyteomics
2) idx = scan_num only works for consecutive scans (no multi-experiment, no polarity switching, no filtering)
3) Only USEFUL when you're parsing the entire file! Nobody knows what scan number 976 corresponds to, making extraction by number useless

# Conclusion

# Acknowledgements

# Data availability

# References

# Supplement